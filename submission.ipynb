{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jasper Driessens 11349026, Jasper Linmans 10249060 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from types import MethodType\n",
    "from itertools import product\n",
    "from random import getrandbits\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relevance:\n",
    "    \"\"\"Statically used class containing the relevance grade enumeration.\"\"\"\n",
    "    N, R, HR = [\n",
    "        \"N \",  # not relevant\n",
    "        \"R \",  # relevant\n",
    "        \"HR\"  # highly relevant\n",
    "    ]\n",
    "\n",
    "    all = [N, R, HR]\n",
    "\n",
    "\n",
    "def quantify(grade):\n",
    "    \"\"\"Assigns a numerical value to a relevance grade.\"\"\"\n",
    "    if grade is Relevance.N:\n",
    "        return 0\n",
    "    if grade is Relevance.R:\n",
    "        return 1\n",
    "    if grade is Relevance.HR:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def relevant(grade):\n",
    "    \"\"\"Tells if a relevance grade counts as relevant (R / HR) or not (N).\"\"\"\n",
    "    return grade is Relevance.R or grade is Relevance.HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discounted_gain_at(k, ranking):\n",
    "    \"\"\"Returns discounted gain resulting from the document at rank `k` in `ranking`. \"\"\"\n",
    "    index = k - 1  # convert 1-based rank to 0-based index\n",
    "    gain = np.power(2, quantify(ranking[index])) - 1  # non-linear gain resulting from document\n",
    "    discount = np.log2(1 + k)  # discount factor\n",
    "    return gain / float(discount)\n",
    "\n",
    "\n",
    "def dcg_at(k, ranking):\n",
    "    \"\"\"Returns DCG at rank `k` for `ranking`. Computes `discounted_gain_at`\n",
    "    at each rank from 1 to and including `k`, and sums these values. \"\"\"\n",
    "    return sum([discounted_gain_at(i + 1, ranking) for i in range(k)])\n",
    "\n",
    "\n",
    "class Ranking:\n",
    "    \"\"\"Ranking objects embody an ordering of Relevance labels and represent either\n",
    "    the result of a query to the E or P algorithm, or an interleaving of those.\"\"\"\n",
    "\n",
    "    total_relevant = 20  # Total amount of relevant (R / HR) documents in collection\n",
    "    persistence = 0.8  # The RBP persistence parameter\n",
    "\n",
    "    def __init__(self, ranking):\n",
    "        \"\"\"Creates a new Ranking object according to ordered Relevance labels `ranking`.\"\"\"\n",
    "        self.ranking = ranking  # Internal ordering of Relevance labels, representing search results\n",
    "        self.n = len(ranking)\n",
    "        self.perfect = [Relevance.HR] * self.n  # Optimum ranking of the same length, for DCG normalization\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Ranking objects are equal when they contain the same relevance label ordering. \"\"\"\n",
    "        return self.ranking == other.ranking if isinstance(other, self.__class__) else NotImplemented\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other) if isinstance(other, self.__class__) else NotImplemented\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.ranking)))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Ranking\" + str(self.ranking)        \n",
    "        \n",
    "    def relevant_at(self, k):\n",
    "        \"\"\"Return amount of relevant (R / HR) documents from ranks 1 to and including `k`.\"\"\"\n",
    "        return sum(relevant(grade) for grade in self.ranking[:k])\n",
    "\n",
    "    def precision_at(self, k):\n",
    "        \"\"\"Return precision at rank `k`: amount of relevant (R / HR) documents from ranks 1 to\n",
    "        and including `k`, divided by total amount of documents in that range (which is `k`).\"\"\"\n",
    "        return self.relevant_at(k) / float(k)\n",
    "\n",
    "    def recall_at(self, k):\n",
    "        \"\"\"Return recall at rank `k`: amount of relevant (R / HR) documents from ranks 1 to\n",
    "        and including `k`, divided by total amount of relevant (R / HR) documents in collection\n",
    "        (given by `total_relevant`). \"\"\"\n",
    "        return self.relevant_at(k) / float(self.total_relevant)\n",
    "\n",
    "    def average_precision(self):\n",
    "        \"\"\"Return average precision of this Ranking: average of `precision_at` evaluated at\n",
    "        each rank where this Ranking has a relevant (H / HR) document.\"\"\"\n",
    "        precisions = [self.precision_at(i + 1) for i in range(self.n) if relevant(self.ranking[i])]\n",
    "        return sum(precisions) / len(precisions) if len(precisions) > 0 else 0\n",
    "\n",
    "    def dcg_at(self, k):\n",
    "        \"\"\"Returns DCG at rank `k` of this Ranking. Wrapper static dcg_at function.\"\"\"\n",
    "        return dcg_at(k, self.ranking)\n",
    "\n",
    "    def ndcg_at(self, k):\n",
    "        \"\"\"Returns normalized DCG at rank `k` of this Ranking. Normalizes by computing\n",
    "        DCG at rank `k` of the best possible ranking (stored in `perfect`) and dividing\n",
    "        the regular (unnormalized) DCG by this value.\"\"\"\n",
    "        return dcg_at(k, self.ranking) / dcg_at(k, self.perfect)\n",
    "\n",
    "    def observation_probability_at(self, k):\n",
    "        return (1 - self.persistence) * np.power(self.persistence)\n",
    "\n",
    "    def rank_biased_precision(self):\n",
    "        return sum([self.ranking[k] * self.observation_probability_at(k) for k in range(self.n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Origin:\n",
    "    P, E = ['P ', 'E ']\n",
    "\n",
    "class RankingPair:\n",
    "    \"\"\"RankingPair objects embody a pair of Ranking objects, representing\n",
    "    the results of the P and E algorithms to a query.\n",
    "\n",
    "    Contains all the delta measure methods, which are not further documented\n",
    "    since they are self-explanatory.\"\"\"\n",
    "\n",
    "    def __init__(self, p, e):\n",
    "        self.p = p  # The results of the P algorithm\n",
    "        self.e = e  # The results of the E algorithm\n",
    "\n",
    "    def delta_precision_at(self, k):\n",
    "        return self.e.precision_at(k) - self.p.precision_at(k)\n",
    "\n",
    "    def delta_recall_at(self, k):\n",
    "        return self.e.recall_at(k) - self.p.recall_at(k)\n",
    "\n",
    "    def delta_average_precision(self):\n",
    "        return self.e.average_precision() - self.p.average_precision()\n",
    "\n",
    "    def delta_dcg_at(self, k):\n",
    "        return self.e.dcg_at(k) - self.p.dcg_at(k)\n",
    "\n",
    "    def delta_ndcg_at(self, k):\n",
    "        return self.e.ndcg_at(k) - self.p.ndcg_at(k)\n",
    "\n",
    "    def delta_rbp(self):\n",
    "        return self.e.rank_biased_precision() - self.p.rank_biased_precision()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"RankingPair[P=\" + str(self.p.ranking) + \", E=\" + str(self.e.ranking) + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "Step 2: Implement Evaluation Measures (15 points)\n",
    "\n",
    "    Implemented in the classes inserted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_rankings(length, grades):\n",
    "    rankings = list(product(grades, repeat=length))\n",
    "    return [Ranking(list(ranking)) for ranking in rankings]\n",
    "\n",
    "\n",
    "def generate_pairs(rankings):\n",
    "    pairs = list(product(rankings, repeat=2))\n",
    "    return [RankingPair(p, e) for p, e in pairs]\n",
    "\n",
    "\n",
    "def generate_all_pairs():\n",
    "    return generate_pairs(generate_rankings(5, Relevance.all))\n",
    "\n",
    "\n",
    "def generate_all_winners(delta_method, parameter=None):\n",
    "    all_pairs = generate_all_pairs()\n",
    "    if parameter is None:\n",
    "        winners = [pair for pair in all_pairs if MethodType(delta_method, pair)() > 0]\n",
    "    else:\n",
    "        winners = [pair for pair in all_pairs if MethodType(delta_method, pair)(parameter) > 0]\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_generator():\n",
    "    all_pairs = generate_all_pairs()\n",
    "    print all_pairs[0]\n",
    "    print all_pairs[-1]\n",
    "    print all_pairs[9235]\n",
    "    print len(all_pairs)\n",
    "\n",
    "def test_winner_generator():\n",
    "    all = generate_all_pairs()\n",
    "    winners = generate_all_winners(RankingPair.delta_recall_at, 5)\n",
    "    amount_of_winners = sum(winner.delta_recall_at(1) > 0 for winner in winners)\n",
    "    amount_of_losers = sum(winner.delta_recall_at(1) < 0 for winner in winners)\n",
    "    amount_of_ties = len(all) - amount_of_losers - amount_of_winners\n",
    "\n",
    "    print \"#all: \" + str(len(all))\n",
    "    print \"#winners: \" + str(len(winners))\n",
    "    print \"winners %: \" + str(len(winners) / float(len(all)) * 100)\n",
    "    print \"winners check: \" + str(amount_of_winners)\n",
    "    print \"losers check: \" + str(amount_of_losers)\n",
    "    print \"ties check: \" + str(amount_of_ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Calculate the ð›¥measure (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_deltas():\n",
    "    ranking1 = Ranking([Relevance.HR, Relevance.N, Relevance.N, Relevance.N, Relevance.N])\n",
    "    ranking2 = Ranking([Relevance.N, Relevance.N, Relevance.N, Relevance.N, Relevance.HR])\n",
    "    pair = RankingPair(ranking1, ranking2)\n",
    "    print \"average precision:       \" + str(pair.delta_average_precision())\n",
    "    print \"cdg at 1:                \" + str(pair.delta_dcg_at(1))\n",
    "    print \"cdg at 3:                \" + str(pair.delta_dcg_at(3))\n",
    "    print \"cdg at 5:                \" + str(pair.delta_dcg_at(5))\n",
    "    print \"ncdg at 1:               \" + str(pair.delta_ndcg_at(1))\n",
    "    print \"ncdg at 3:               \" + str(pair.delta_ndcg_at(3))\n",
    "    print \"ncdg at 5:               \" + str(pair.delta_ndcg_at(5))\n",
    "    print \"precision at 1:          \" + str(pair.delta_precision_at(1))\n",
    "    print \"precision at 3:          \" + str(pair.delta_precision_at(3))\n",
    "    print \"precision at 5:          \" + str(pair.delta_precision_at(5))\n",
    "    print \"recall at 1:             \" + str(pair.delta_recall_at(1))\n",
    "    print \"recall at 3:             \" + str(pair.delta_recall_at(3))\n",
    "    print \"recall at 5:             \" + str(pair.delta_recall_at(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Interleaving:\n",
    "    def __init__(self, ranking, origins):\n",
    "        self.ranking = ranking\n",
    "        self.origins = origins\n",
    "\n",
    "        \n",
    "class Winner:\n",
    "    P, E, T = ['P', 'E', 'T']\n",
    "\n",
    "    \n",
    "class Interleaver:\n",
    "    def __init__(self):\n",
    "        self.click_model = None\n",
    "\n",
    "    def run_simulation(self, pair):\n",
    "        ranking, origins = self.interleave(pair)\n",
    "        clicks = self.click_model.simulate_clicks_on(ranking)\n",
    "        production_clicks = 0\n",
    "        experiment_clicks = 0\n",
    "\n",
    "        for i in range(len(ranking)):\n",
    "            if clicks[i]:\n",
    "                if origins[i] is Origin.P:\n",
    "                    production_clicks += 1\n",
    "                else:\n",
    "                    experiment_clicks += 1\n",
    "\n",
    "        return production_clicks, experiment_clicks\n",
    "    \n",
    "    def evaluate(self, pair):\n",
    "        production_clicks, experiment_clicks = self.run_simulation(pair)\n",
    "        if experiment_clicks == production_clicks:\n",
    "            return Winner.T\n",
    "        else:\n",
    "            return Winner.P if production_clicks > experiment_clicks else Winner.E\n",
    "\n",
    "        \n",
    "class BalancedInterleaver(Interleaver):\n",
    "    def __init__(self):\n",
    "        Interleaver.__init__(self)\n",
    "\n",
    "    def interleave(self, pair):\n",
    "        length = min(len(pair.p.ranking), len(pair.e.ranking))\n",
    "        production_first = getrandbits(1)\n",
    "\n",
    "        origins = ([Origin.P, Origin.E] if production_first else [Origin.E, Origin.P]) * length\n",
    "        ranking = [None] * length * 2\n",
    "\n",
    "        ranking[int(not production_first)::2] = pair.p.ranking\n",
    "        ranking[int(production_first)::2] = pair.e.ranking\n",
    "\n",
    "        return ranking, origins\n",
    "\n",
    "\n",
    "class TeamDraftInterleaver(Interleaver):\n",
    "    def __init__(self):\n",
    "        Interleaver.__init__(self)\n",
    "\n",
    "    def interleave(self, pair):\n",
    "        length = min(len(pair.p.ranking), len(pair.e.ranking))\n",
    "\n",
    "        origins, ranking = [], []\n",
    "\n",
    "        for i in range(length):\n",
    "            if getrandbits(1):\n",
    "                origins.extend([Origin.P, Origin.E])\n",
    "                ranking.extend([pair.p.ranking[i], pair.e.ranking[i]])\n",
    "            else:\n",
    "                origins.extend([Origin.E, Origin.P])\n",
    "                ranking.extend([pair.e.ranking[i], pair.p.ranking[i]])\n",
    "\n",
    "        return ranking, origins\n",
    "\n",
    "\n",
    "class ProbabilisticInterleaver(Interleaver):\n",
    "    tau = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        Interleaver.__init__(self)\n",
    "\n",
    "    def interleave(self, pair):\n",
    "        length = min(len(pair.p.ranking), len(pair.e.ranking))\n",
    "\n",
    "        origins, ranking = [], []\n",
    "\n",
    "        remaining_p = pair.p.ranking[:]\n",
    "        remaining_e = pair.e.ranking[:]\n",
    "\n",
    "        for i in range(length):\n",
    "            if getrandbits(1):\n",
    "                ranking.append(self.sample_element_from(remaining_p))\n",
    "                ranking.append(self.sample_element_from(remaining_e))\n",
    "                origins.extend([Origin.P, Origin.E])\n",
    "            else:\n",
    "                ranking.append(self.sample_element_from(remaining_e))\n",
    "                ranking.append(self.sample_element_from(remaining_p))\n",
    "                origins.extend([Origin.E, Origin.P])\n",
    "\n",
    "        return ranking, origins\n",
    "\n",
    "    def sample_element_from(self, remaining_list):\n",
    "        remaining_amount = len(remaining_list)\n",
    "        probabilities = self.softmax_probabilities(remaining_amount)\n",
    "        sampled_index = np.random.choice(range(remaining_amount), p=probabilities)\n",
    "        popped = remaining_list.pop(sampled_index)\n",
    "        return popped\n",
    "\n",
    "    def softmax_probabilities(self, length):\n",
    "        unnormalized = [1 / float(np.power(rank, self.tau)) for rank in range(1, length + 1)]\n",
    "        normalization_factor = float(sum(unnormalized))\n",
    "        normalized = [x / normalization_factor for x in unnormalized]\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Implement User Clicks Simulation (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_yandex_file(file_name):\n",
    "    queries = []\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        for line in f.read().splitlines():\n",
    "            if 'Q' in line:\n",
    "                session_id, query_id, ranking = parse_query(line)\n",
    "                query = next((q for q in queries if q.session_id == session_id and q.id == query_id), None)\n",
    "\n",
    "                if query is None:\n",
    "                    queries.append(Query(session_id, query_id, ranking))\n",
    "                else:\n",
    "                    query.ranking += ranking\n",
    "\n",
    "            else:\n",
    "                session_id, url_id = parse_click(line)\n",
    "                query = next(q for q in reversed(queries) if url_id in q.ranking and session_id == q.session_id)\n",
    "                query.clicks.append(query.ranking.index(url_id))\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def parse_query(string):\n",
    "    split = string.split()\n",
    "    session_id = split[0]\n",
    "    query_id = split[3]\n",
    "    ranking = split[5:]\n",
    "    return session_id, query_id, ranking\n",
    "\n",
    "\n",
    "def parse_click(string):\n",
    "    split = string.split()\n",
    "    session_id = split[0]\n",
    "    url_id = split[3]\n",
    "    return session_id, url_id\n",
    "\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, session_id, query_id, ranking):\n",
    "        self.session_id = session_id\n",
    "        self.id = query_id\n",
    "        self.ranking = ranking\n",
    "        self.clicks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClickModel(object):\n",
    "    def __init__(self):\n",
    "        self.params_initialized = False\n",
    "\n",
    "    def simulate_clicks_on(self, ranking):\n",
    "        probabilities = self.probabilities(ranking)\n",
    "        clicks = [random() < p for p in probabilities]\n",
    "        return clicks\n",
    "\n",
    "\n",
    "class RandomClickModel(ClickModel):\n",
    "    yandex_file_name = 'YandexRelPredChallenge.txt'\n",
    "    params_file_name = 'RandomClickModelParams.txt'\n",
    "\n",
    "    def __init__(self):\n",
    "        ClickModel.__init__(self)\n",
    "        self.rho = None\n",
    "\n",
    "    def learn(self):\n",
    "        queries = parse_yandex_file(self.yandex_file_name)\n",
    "\n",
    "        results_count = 0\n",
    "        click_count = 0\n",
    "\n",
    "        for query in queries:\n",
    "            results_count += len(query.ranking)\n",
    "            click_count += len(query.clicks)\n",
    "\n",
    "        self.rho = click_count / float(results_count)\n",
    "\n",
    "        with open(self.params_file_name, 'w+') as f:\n",
    "            f.write(str(self.rho))\n",
    "\n",
    "        self.params_initialized = True\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.params_file_name) as f:\n",
    "            self.rho = float(f.readline())\n",
    "        self.params_initialized = True\n",
    "\n",
    "    def probabilities(self, ranking):\n",
    "        if not self.params_initialized:\n",
    "            raise AssertionError('Parameters have not been initialized.')\n",
    "        return [self.rho] * len(ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Simulate Interleaving Experiment (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Results and Analysis (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision + Balanced Interleaver + Random Click Model: 0.499419779891\n",
      "Average Precision + Probabil Interleaver + Random Click Model: 0.499681814779\n",
      "Normalized DCG @5 + Balanced Interleaver + Random Click Model: 0.498706427015\n",
      "Normalized DCG    + Balanced Interleaver + Random Click Model: 0.504578567538\n"
     ]
    }
   ],
   "source": [
    "def compare(pairs, interleaver, click_model):\n",
    "    click_model.load()\n",
    "    interleaver.click_model = click_model\n",
    "    production_wins, experiment_wins, ties = 0, 0, 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        winner = interleaver.evaluate(pair)\n",
    "        if winner is Winner.P:\n",
    "            production_wins += 1\n",
    "        elif winner is Winner.E:\n",
    "            experiment_wins += 1\n",
    "        else:\n",
    "            ties += 1\n",
    "\n",
    "    return (experiment_wins + 0.5 * ties) / float(len(pairs))\n",
    "\n",
    "def analyse():\n",
    "    pairs1 = generate_all_winners(RankingPair.delta_average_precision)\n",
    "    inter1 = BalancedInterleaver()\n",
    "    model1 = RandomClickModel()\n",
    "    combo1 = compare(pairs1, inter1, model1)\n",
    "    print \"Average Precision + Balanced Interleaver + Random Click Model: \" + str(combo1)\n",
    "\n",
    "    pairs2 = generate_all_winners(RankingPair.delta_average_precision)\n",
    "    inter2 = ProbabilisticInterleaver()\n",
    "    model2 = RandomClickModel()\n",
    "    combo2 = compare(pairs2, inter2, model2)\n",
    "    print \"Average Precision + Probabil Interleaver + Random Click Model: \" + str(combo2)\n",
    "\n",
    "    pairs3 = generate_all_winners(RankingPair.delta_ndcg_at, 5)\n",
    "    inter3 = BalancedInterleaver()\n",
    "    model3 = RandomClickModel()\n",
    "    combo3 = compare(pairs3, inter3, model3)\n",
    "    print \"Normalized DCG @5 + Balanced Interleaver + Random Click Model: \" + str(combo3)\n",
    "\n",
    "    pairs4 = generate_all_winners(RankingPair.delta_ndcg_at, 5)\n",
    "    inter4 = BalancedInterleaver()\n",
    "    model4 = RandomClickModel()\n",
    "    combo4 = compare(pairs4, inter4, model4)\n",
    "    print \"Normalized DCG    + Balanced Interleaver + Random Click Model: \" + str(combo4)\n",
    "    \n",
    "analyse()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}