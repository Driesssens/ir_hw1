{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jasper Driessens 11349026, Jasper Linmans 10249060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "Please note that although the comments in this notebook are somewhat scarce, almost all parts of the code have been documented and contain relevant design choices, insights, assumptions et cetera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from types import MethodType\n",
    "from itertools import product\n",
    "from random import getrandbits\n",
    "from random import random\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Origin:\n",
    "    \"\"\"Statically used class containing the origin enumeration. Used by interleavers\n",
    "    to keep track of which algorithm provided each document in the interleaved list.\"\"\"\n",
    "    P, E = [\n",
    "        'P ',  # indicates a document originated from the production algorithm\n",
    "        'E '  # indicates a document originated from the experimental algorithm\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relevance:\n",
    "    \"\"\"Statically used class containing the relevance grade enumeration.\"\"\"\n",
    "    N, R, HR = [\n",
    "        \"N \",  # not relevant\n",
    "        \"R \",  # relevant\n",
    "        \"HR\"  # highly relevant\n",
    "    ]\n",
    "\n",
    "    all = [N, R, HR]\n",
    "\n",
    "\n",
    "def quantify(grade):\n",
    "    \"\"\"Assigns a numerical value to a relevance grade.\"\"\"\n",
    "    if grade is Relevance.N:\n",
    "        return 0\n",
    "    if grade is Relevance.R:\n",
    "        return 1\n",
    "    if grade is Relevance.HR:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def relevant(grade):\n",
    "    \"\"\"Tells if a relevance grade counts as relevant (R / HR) or not (N).\"\"\"\n",
    "    return grade is Relevance.R or grade is Relevance.HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Winner:\n",
    "    \"\"\"Statically used class containing the winner enumeration. Used by interleaving methods\n",
    "    to indicate which algorithm won an online evaluation.\"\"\"\n",
    "    P, E, T = [\n",
    "        'P',  # The production algorithm won this evaluation.\n",
    "        'E',  # The experimental algorithm won this evaluation.\n",
    "        'T'  # This evaluation resulted in a tie.\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1, 2 and 3\n",
    "\n",
    "Here, we generate pairs of relevance grade labels, which simulate the retrieval results to imaginary queries of the production (P) and experimental (E) algorithms.\n",
    "\n",
    "We use a class Ranking and a class RankingPair to embody those things.\n",
    "\n",
    "The Ranking class contains all the offline evaluation measure methods - we arbitrarily decided to go for Average Precision, DCG @ 5 and nDCG @ 5.\n",
    "\n",
    "The RankingPair objects compute the delta measures for the pairs they embody.\n",
    "\n",
    "Lastly, we have some generator methods that combinatorically generate all possible ranking pairs, and select those for which E outperforms P according to some offline evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discounted_gain_at(k, ranking):\n",
    "    \"\"\"Returns discounted gain resulting from the document at rank `k` in `ranking`. \"\"\"\n",
    "    index = k - 1  # convert 1-based rank to 0-based index\n",
    "    gain = np.power(2, quantify(ranking[index])) - 1  # non-linear gain resulting from document\n",
    "    discount = np.log2(1 + k)  # discount factor\n",
    "    return gain / float(discount)\n",
    "\n",
    "\n",
    "def dcg_at(k, ranking):\n",
    "    \"\"\"Returns DCG at rank `k` for `ranking`. Computes `discounted_gain_at`\n",
    "    at each rank from 1 to and including `k`, and sums these values. \"\"\"\n",
    "    return sum([discounted_gain_at(i + 1, ranking) for i in range(k)])\n",
    "\n",
    "\n",
    "class Ranking:\n",
    "    \"\"\"Ranking objects embody an ordering of Relevance labels and represent either\n",
    "    the result of a query to the E or P algorithm, or an interleaving of those.\"\"\"\n",
    "\n",
    "    total_relevant = 20  # Total amount of relevant (R / HR) documents in collection\n",
    "    persistence = 0.8  # The RBP persistence parameter\n",
    "\n",
    "    def __init__(self, ranking):\n",
    "        \"\"\"Creates a new Ranking object according to ordered Relevance labels `ranking`.\"\"\"\n",
    "        self.ranking = ranking  # Internal ordering of Relevance labels, representing search results\n",
    "        self.n = len(ranking)\n",
    "        self.perfect = [Relevance.HR] * self.n  # Optimum ranking of the same length, for DCG normalization\n",
    "\n",
    "    def relevant_at(self, k):\n",
    "        \"\"\"Return amount of relevant (R / HR) documents from ranks 1 to and including `k`.\"\"\"\n",
    "        return sum(relevant(grade) for grade in self.ranking[:k])\n",
    "\n",
    "    def precision_at(self, k):\n",
    "        \"\"\"Return precision at rank `k`: amount of relevant (R / HR) documents from ranks 1 to\n",
    "        and including `k`, divided by total amount of documents in that range (which is `k`).\"\"\"\n",
    "        return self.relevant_at(k) / float(k)\n",
    "\n",
    "    def recall_at(self, k):\n",
    "        \"\"\"Return recall at rank `k`: amount of relevant (R / HR) documents from ranks 1 to\n",
    "        and including `k`, divided by total amount of relevant (R / HR) documents in collection\n",
    "        (given by `total_relevant`). \"\"\"\n",
    "        return self.relevant_at(k) / float(self.total_relevant)\n",
    "\n",
    "    def average_precision(self):\n",
    "        \"\"\"Return average precision of this Ranking: average of `precision_at` evaluated at\n",
    "        each rank where this Ranking has a relevant (H / HR) document.\"\"\"\n",
    "        precisions = [self.precision_at(i + 1) for i in range(self.n) if relevant(self.ranking[i])]\n",
    "        return sum(precisions) / len(precisions) if len(precisions) > 0 else 0\n",
    "\n",
    "    def dcg_at(self, k):\n",
    "        \"\"\"Returns DCG at rank `k` of this Ranking. Wrapper static dcg_at function.\"\"\"\n",
    "        return dcg_at(k, self.ranking)\n",
    "\n",
    "    def ndcg_at(self, k):\n",
    "        \"\"\"Returns normalized DCG at rank `k` of this Ranking. Normalizes by computing\n",
    "        DCG at rank `k` of the best possible ranking (stored in `perfect`) and dividing\n",
    "        the regular (unnormalized) DCG by this value.\"\"\"\n",
    "        return dcg_at(k, self.ranking) / dcg_at(k, self.perfect)\n",
    "\n",
    "    def observation_probability_at(self, k):\n",
    "        return (1 - self.persistence) * np.power(self.persistence)\n",
    "\n",
    "    def rank_biased_precision(self):\n",
    "        return sum([self.ranking[k] * self.observation_probability_at(k) for k in range(self.n)])\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Ranking objects are equal when they contain the same relevance label ordering. \"\"\"\n",
    "        return self.ranking == other.ranking if isinstance(other, self.__class__) else NotImplemented\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other) if isinstance(other, self.__class__) else NotImplemented\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.ranking)))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Ranking\" + str(self.ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankingPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RankingPair:\n",
    "    \"\"\"RankingPair objects embody a pair of Ranking objects, representing\n",
    "    the results of the P and E algorithms to a query.\n",
    "\n",
    "    Contains all the delta measure methods, which are not further documented\n",
    "    since they are self-explanatory.\"\"\"\n",
    "\n",
    "    def __init__(self, p, e):\n",
    "        self.p = p  # The results of the P algorithm\n",
    "        self.e = e  # The results of the E algorithm\n",
    "\n",
    "    def delta_precision_at(self, k):\n",
    "        return self.e.precision_at(k) - self.p.precision_at(k)\n",
    "\n",
    "    def delta_recall_at(self, k):\n",
    "        return self.e.recall_at(k) - self.p.recall_at(k)\n",
    "\n",
    "    def delta_average_precision(self):\n",
    "        return self.e.average_precision() - self.p.average_precision()\n",
    "\n",
    "    def delta_dcg_at(self, k):\n",
    "        return self.e.dcg_at(k) - self.p.dcg_at(k)\n",
    "\n",
    "    def delta_ndcg_at(self, k):\n",
    "        return self.e.ndcg_at(k) - self.p.ndcg_at(k)\n",
    "\n",
    "    def delta_rbp(self):\n",
    "        return self.e.rank_biased_precision() - self.p.rank_biased_precision()\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"RankingPair[P=\" + str(self.p.ranking) + \", E=\" + str(self.e.ranking) + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators and selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_rankings(length, grades):\n",
    "    \"\"\"\"Generates all combinatorial possibilities of ranking lists existing of\n",
    "    the relevance grade labels in `grades`, being of length `length.\"\"\"\n",
    "    rankings = list(product(grades, repeat=length))  # all combinatorial possibilities\n",
    "    return [Ranking(list(ranking)) for ranking in rankings]  # turn into Ranking objects\n",
    "\n",
    "\n",
    "def generate_pairs(rankings):\n",
    "    \"\"\"Given a set of `rankings`, generates all possible pairs. \"\"\"\n",
    "    pairs = list(product(rankings, repeat=2))\n",
    "    return [RankingPair(p, e) for p, e in pairs]\n",
    "\n",
    "\n",
    "def generate_all_pairs():\n",
    "    \"\"\"\"Generates all possible pairs of rankings of length 5 containing\n",
    "    relevance labels N, R and HR. \"\"\"\n",
    "    return generate_pairs(generate_rankings(5, Relevance.all))\n",
    "\n",
    "\n",
    "def generate_all_winners(delta_method, parameter=None):\n",
    "    \"\"\"\"Generates all pairs, but throws away those pairs for which the experimental algorithm\n",
    "    is not the winner according to offline evaluation metric `delta_method` (possibly parameterized\n",
    "    with `parameter`, for instance in the case of 'recall at 5'. \"\"\"\n",
    "    all_pairs = generate_all_pairs()\n",
    "    if parameter is None:\n",
    "        winners = [pair for pair in all_pairs if MethodType(delta_method, pair)() > 0]\n",
    "    else:\n",
    "        winners = [pair for pair in all_pairs if MethodType(delta_method, pair)(parameter) > 0]\n",
    "    return winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: interleaving\n",
    "\n",
    "For interleaving methods, we decided to go with Balanced Interleaving and Probabilistic Interleaving. The code is fairly well-documented, so further explanation in this cell is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaver base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Interleaver:\n",
    "    \"\"\"Base class for interleavers. Stores the click model that it uses for simulations in `click_model`.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.click_model = None  # The click model to use in simulations.\n",
    "\n",
    "    def run_simulation(self, pair):\n",
    "        \"\"\"\"Runs one simulation of an online evaluation. Returns the number of clicks that the E and P algorithms have received.\"\"\"\n",
    "        ranking, origins = self.interleave(pair)\n",
    "        clicks = self.click_model.simulate_clicks_on(ranking)\n",
    "        production_clicks = 0\n",
    "        experiment_clicks = 0\n",
    "\n",
    "        for i in range(len(ranking)):\n",
    "            if clicks[i]:\n",
    "                if origins[i] is Origin.P:  # Did the production algorithm provide this document to the interleaved list?\n",
    "                    production_clicks += 1\n",
    "                else:  # Or was it the experimental algorithm?\n",
    "                    experiment_clicks += 1\n",
    "\n",
    "        return production_clicks, experiment_clicks\n",
    "\n",
    "    def evaluate(self, pair):\n",
    "        \"\"\"\"Runs one simulation of an online evaluation. Returns which algorithm won, or that there was a tie.\"\"\"\n",
    "        production_clicks, experiment_clicks = self.run_simulation(pair)  # Simulate clicks\n",
    "        if experiment_clicks == production_clicks:\n",
    "            return Winner.T  # This means there was a tie.\n",
    "        else:\n",
    "            return Winner.P if production_clicks > experiment_clicks else Winner.E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BalancedInterleaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BalancedInterleaver(Interleaver):\n",
    "    \"\"\"Implements the balanced interleaving method.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        Interleaver.__init__(self)\n",
    "\n",
    "    def interleave(self, pair):\n",
    "        \"\"\"Performs the balanced interleaving method.\"\"\"\n",
    "        length = min(len(pair.p.ranking), len(pair.e.ranking))\n",
    "        production_first = getrandbits(1)  # Flip a coin. '1' means the production algorithm may provide a document first.\n",
    "\n",
    "        #  Stores which algorithm provided the document at each position of the resulting interleaved list\n",
    "        origins = ([Origin.P, Origin.E] if production_first else [Origin.E, Origin.P]) * length\n",
    "\n",
    "        #  The next three lines do a Python slicing trick to interleave the lists alternating between P and E.\n",
    "        ranking = [None] * length * 2\n",
    "        ranking[int(not production_first)::2] = pair.p.ranking\n",
    "        ranking[int(production_first)::2] = pair.e.ranking\n",
    "\n",
    "        return ranking, origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProbabilisticInterleaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProbabilisticInterleaver(Interleaver):\n",
    "    \"\"\"Implements the probabilistic interleaving method.\"\"\"\n",
    "\n",
    "    tau = 3  # The tau model parameter. Chosen to be 3 as recommended in the paper.\n",
    "\n",
    "    def __init__(self):\n",
    "        Interleaver.__init__(self)\n",
    "\n",
    "    def interleave(self, pair):\n",
    "        \"\"\"Performs the probabilistic interleaving method.\"\"\"\n",
    "        length = min(len(pair.p.ranking), len(pair.e.ranking))\n",
    "\n",
    "        # `origins` stores which algorithm provided the document at each position of the resulting interleaved list\n",
    "        origins, ranking = [], []\n",
    "\n",
    "        # Copy the rankings to keep track of the documents we still have to interleave\n",
    "        remaining_p = pair.p.ranking[:]\n",
    "        remaining_e = pair.e.ranking[:]\n",
    "\n",
    "        for i in range(length):\n",
    "            if getrandbits(1):  # flip a coin to see which algorithm goes first\n",
    "                ranking.append(self.sample_element_from(remaining_p))  # P goes first; stochastically pick a remaining document\n",
    "                ranking.append(self.sample_element_from(remaining_e))  # Then do the same for E\n",
    "                origins.extend([Origin.P, Origin.E])  # Keep track of the fact that P went first in this round\n",
    "            else:\n",
    "                ranking.append(self.sample_element_from(remaining_e))\n",
    "                ranking.append(self.sample_element_from(remaining_p))\n",
    "                origins.extend([Origin.E, Origin.P])\n",
    "\n",
    "        return ranking, origins\n",
    "\n",
    "    def sample_element_from(self, remaining_list):\n",
    "        \"\"\"Stochastically picks (and removes) a document from the `remaining_list`, according to softmax probabilities.\"\"\"\n",
    "        remaining_amount = len(remaining_list)\n",
    "        probabilities = self.softmax_probabilities(remaining_amount)\n",
    "        sampled_index = np.random.choice(range(remaining_amount), p=probabilities)\n",
    "        popped = remaining_list.pop(sampled_index)\n",
    "        return popped\n",
    "\n",
    "    def softmax_probabilities(self, length):\n",
    "        \"\"\"Computes the softmax probability distribution for a document list of `length`.\"\"\"\n",
    "        unnormalized = [1 / float(np.power(rank, self.tau)) for rank in range(1, length + 1)]\n",
    "        normalization_factor = float(sum(unnormalized))\n",
    "        normalized = [x / normalization_factor for x in unnormalized]  # normalize all probabilities to make it a distribution\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: click models\n",
    "\n",
    "We have chose the Random Click Model and the Simplified Dependent Click Model. To train these, we have used a Yandex log file, for which we have written a parser. Please see the code comments for relevant all motivations / assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yandex parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_yandex_file(file_name):\n",
    "    \"\"\"\"Parses the Yandex log file.\n",
    "\n",
    "    Makes the following assumptions:\n",
    "    - Within a session, a click on a url_id was on the most recent query that returned that url_id.\n",
    "    - Within a session, if a query_id appears a second time with different results, this is\n",
    "        interpreted as page 2, and the results get appended to the query\n",
    "    - Within a session, if a query_id appears a second time with overlapping results,\n",
    "        this is interpreted as a new query\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        for line in f.read().splitlines():\n",
    "            if 'Q' in line:\n",
    "                parsed_session_id, parsed_query_id, parsed_ranking = parse_query(line)\n",
    "                query = next(\n",
    "                    (\n",
    "                        q for q in reversed(queries) if\n",
    "                        (\n",
    "                            q.session_id == parsed_session_id\n",
    "                            and q.id == parsed_query_id\n",
    "                            and not any(url_id in q.ranking for url_id in parsed_ranking)\n",
    "                        )\n",
    "                    ), None)\n",
    "\n",
    "                if query is None:\n",
    "                    queries.append(Query(parsed_session_id, parsed_query_id, parsed_ranking))\n",
    "                else:\n",
    "                    query.ranking += parsed_ranking\n",
    "\n",
    "            else:\n",
    "                parsed_session_id, parsed_url_id = parse_click(line)\n",
    "                query = next((q for q in reversed(queries) if parsed_url_id in q.ranking and parsed_session_id == q.session_id), None)\n",
    "                query.clicks.append(query.ranking.index(parsed_url_id) + 1)\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def parse_query(string):\n",
    "    split = string.split()\n",
    "    session_id = split[0]\n",
    "    query_id = split[3]\n",
    "    ranking = split[5:]\n",
    "    return session_id, query_id, ranking\n",
    "\n",
    "\n",
    "def parse_click(string):\n",
    "    split = string.split()\n",
    "    session_id = split[0]\n",
    "    url_id = split[3]\n",
    "    return session_id, url_id\n",
    "\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, session_id, query_id, ranking):\n",
    "        self.session_id = session_id\n",
    "        self.id = query_id\n",
    "        self.ranking = ranking\n",
    "        self.clicks = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Query[{0}:{1:4} ranking={2} clicks={3}]\".format(self.session_id, self.id, self.ranking, self.clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClickModel base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClickModel(object):\n",
    "    \"\"\"\"Base class for click models.\"\"\"\n",
    "\n",
    "    yandex_file_name = 'YandexRelPredChallenge.txt'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params_initialized = False  # Safety check: simulation methods are blocked as long as parameters are not initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomClickModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomClickModel(ClickModel):\n",
    "    params_file_name = 'RandomClickModelParams.txt'  # File to store learned parameters\n",
    "\n",
    "    def __init__(self):\n",
    "        ClickModel.__init__(self)\n",
    "        self.rho = None  # Model parameter representing the single click probability for every document\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\"Learns rho parameter from a Yandex log file. Stores it in `params_file_name` afterwards.\"\"\"\n",
    "        queries = parse_yandex_file(self.yandex_file_name)\n",
    "\n",
    "        results_count = 0  # Total amount of results returned by all queries in log file\n",
    "        click_count = 0  # Total amount of clicks in the whole log file\n",
    "\n",
    "        for query in queries:\n",
    "            results_count += len(query.ranking)\n",
    "            click_count += len(query.clicks)\n",
    "\n",
    "        self.rho = click_count / float(results_count)\n",
    "\n",
    "        with open(self.params_file_name, 'w+') as f:\n",
    "            f.write(str(self.rho))  # Storing learned parameter in file\n",
    "\n",
    "        self.params_initialized = True  # We can now run simulations\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\"Loads rho parameter from `params_file_name` if it was previously learned and stored there.\"\"\"\n",
    "        with open(self.params_file_name) as f:\n",
    "            self.rho = float(f.readline())\n",
    "        self.params_initialized = True  # We can now run simulations\n",
    "\n",
    "    def probabilities(self, ranking):\n",
    "        \"\"\"\"Given `ranking`, returns click probabilities for each document in the list.\"\"\"\n",
    "        if not self.params_initialized:\n",
    "            raise AssertionError('Parameters have not been initialized.')\n",
    "        return [self.rho] * len(ranking)  # It is simply the same for each document\n",
    "\n",
    "    def simulate_clicks_on(self, ranking):\n",
    "        \"\"\"\"Given `ranking`, stochastically decides for each document whether it was clicked.\"\"\"\n",
    "        probabilities = self.probabilities(ranking)  # First, get probabilities\n",
    "        clicks = [random() < p for p in probabilities]  # Then, sample yes / no from those probabilities\n",
    "        return clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimplifiedDependentClickModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimplifiedDependentClickModel(ClickModel):\n",
    "    params_file_name = 'SimplifiedDependentClickModelParams.txt'  # File to store learned parameters\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\"Initializes two parameter dictionaries. First, `satisfaction_at`, which contains the satisfaction parameter (inverse\n",
    "        continuation parameter) for each rank. Second, `attractiveness_of`, which contains the attractiveness parameter per\n",
    "        relevance grade. We have chosen to assign 0 to N, 0.5 to R and 1 to HR. \"\"\"\n",
    "        ClickModel.__init__(self)\n",
    "        self.satisfaction_at = {}\n",
    "        max_relevance_quantity = float(max([quantify(grade) for grade in Relevance.all]))\n",
    "        self.attractiveness_of = {grade: quantify(grade) / max_relevance_quantity for grade in Relevance.all}\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Leans the satisfaction parameters (inverse continuation parameters) at each rank from the Yandex log file.\n",
    "         Stores it in `params_file_name` afterwards. In the terminology below, with 'a satisfaction', we mean a click\n",
    "         that was the final click of the query.\"\"\"\n",
    "        queries = parse_yandex_file(self.yandex_file_name)\n",
    "\n",
    "        all_click_lists = [query.clicks for query in queries if query.clicks]\n",
    "        all_clicks = [click for click_list in all_click_lists for click in click_list]\n",
    "        all_satisfactions = [max(click_list) for click_list in all_click_lists]\n",
    "\n",
    "        click_count_at_rank = Counter(all_clicks)  # Counts, per rank, over the whole log file, the number of clicks on that rank\n",
    "        satisfaction_count_at_rank = Counter(all_satisfactions)  # Counts, per rank, over the whole log file, the number of satisfactions on that rank\n",
    "\n",
    "        # This line computes the satisfaction (inverse continuation) parameters for each rank that has ever been the last clicked rank of a query\n",
    "        self.satisfaction_at = {rank: satisfaction_count / float(click_count_at_rank[rank]) for rank, satisfaction_count in satisfaction_count_at_rank.items()}\n",
    "\n",
    "        with open(self.params_file_name, 'w+') as f:\n",
    "            f.write(str(self.satisfaction_at))  # Storing learned parameter in file\n",
    "\n",
    "        self.params_initialized = True  # We can now run simulations\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\"Loads satisfaction parameters (inverse continuation parameters) from `params_file_name` if they were previously learned and stored there.\"\"\"\n",
    "        with open(self.params_file_name) as f:\n",
    "            self.satisfaction_at = literal_eval(f.readline())\n",
    "        self.params_initialized = True  # We can now run simulations\n",
    "\n",
    "    def probabilities(self, ranking):\n",
    "        \"\"\"\"Given `ranking`, returns click and satisfaction probabilities for each document in the list.\"\"\"\n",
    "        if not self.params_initialized:\n",
    "            raise AssertionError('Parameters have not been initialized.')\n",
    "\n",
    "        click_probabilities = [self.attractiveness_of[grade] for grade in ranking]\n",
    "        satisfaction_probabilities = [self.satisfaction_at[index + 1] for index in range(len(ranking))]\n",
    "\n",
    "        return click_probabilities, satisfaction_probabilities\n",
    "\n",
    "    def simulate_clicks_on(self, ranking):\n",
    "        \"\"\"\"Given `ranking`, stochastically decides for each document whether it was clicked. Iterates through the\n",
    "        ranking list according to the model: at each document, stochastically decided whether it was clicked, and if\n",
    "        it was, stochastically decides whether the user was satisfied. If the user was satisfied, stop. \"\"\"\n",
    "        click_probabilities, satisfaction_probabilities = self.probabilities(ranking)\n",
    "        clicks = [False] * len(ranking)\n",
    "\n",
    "        for i in range(len(ranking)):\n",
    "            if random() < click_probabilities[i]:  # Was the document clicked?\n",
    "                clicks[i] = True\n",
    "                if random() < satisfaction_probabilities[i]:  # Was the user satisfied?\n",
    "                    return clicks  # Immediately stop if satisfied\n",
    "\n",
    "        return clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(pairs, interleaver, click_model):\n",
    "    \"\"\"\"The final evaluation method. Given a set of query ranking pairs for the E and P algorithms,\n",
    "     and given an interleaving method, and given a click model, simulations an online evaluation.\n",
    "     For each query ranking pair, one interleaving is presented to 'the user', whom the click\n",
    "     model is modelling. Based on the clicks, the algorithms receive credit. Then, the\n",
    "     'delta_AB' metric from 'Large-Scale Validation and Analysis of Interleaved Search Evaluation'\n",
    "     is used to finally measure the preference an online evaluation expresses for one of the algorithms.\n",
    "\n",
    "     We adjusted the metric in two ways:\n",
    "     - we don't disregard evaluations with zero clicks\n",
    "     - we make it range from -100% to 100%, instead of -50% to 50%\n",
    "     \"\"\"\n",
    "\n",
    "    # Loads the click model parameters (given they have been learned\n",
    "    # before). Change to `learn` to deduce from the Yandex log file.\n",
    "    click_model.load()\n",
    "\n",
    "    interleaver.click_model = click_model\n",
    "    production_wins, experiment_wins, ties = 0, 0, 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        winner = interleaver.evaluate(pair)\n",
    "        if winner is Winner.P:\n",
    "            production_wins += 1\n",
    "        elif winner is Winner.E:\n",
    "            experiment_wins += 1\n",
    "        else:\n",
    "            ties += 1\n",
    "\n",
    "    # Compute and return the adjusted delta_AB metric\n",
    "    return ((experiment_wins + 0.5 * ties) / float(len(pairs)) - 0.5) * 200\n",
    "\n",
    "\n",
    "def analyse():\n",
    "    \"\"\"\"Performs the analysis for each of the 12 model combinations.\"\"\"\n",
    "    print \"The following models express a preference in % for the experimental algorithm on the following data sets:\"\n",
    "    print\n",
    "\n",
    "    print \"     On the data set where the experimental algorithm wins on all pairs according to the average precision:\"\n",
    "    data_set_1 = generate_all_winners(RankingPair.delta_average_precision)\n",
    "\n",
    "    print \"         Balanced Interleaving with RCM =       {0}%\".format(compare(data_set_1, BalancedInterleaver(), RandomClickModel()))\n",
    "    print \"         Probabilistic Interleaving with RCM =  {0}%\".format(compare(data_set_1, ProbabilisticInterleaver(), RandomClickModel()))\n",
    "    print \"         Balanced Interleaving with SDCM =      {0}%\".format(compare(data_set_1, BalancedInterleaver(), SimplifiedDependentClickModel()))\n",
    "    print \"         Probabilistic Interleaving with SDCM = {0}%\".format(compare(data_set_1, ProbabilisticInterleaver(), SimplifiedDependentClickModel()))\n",
    "    print \"     This data set contained {0} pairs.\".format(len(data_set_1))\n",
    "    print\n",
    "\n",
    "    print \"     On the data set where the experimental algorithm wins on all pairs according to DCG at rank 5:\"\n",
    "    data_set_2 = generate_all_winners(RankingPair.delta_dcg_at, 5)\n",
    "\n",
    "    print \"         Balanced Interleaving with RCM =       {0}%\".format(compare(data_set_2, BalancedInterleaver(), RandomClickModel()))\n",
    "    print \"         Probabilistic Interleaving with RCM =  {0}%\".format(compare(data_set_2, ProbabilisticInterleaver(), RandomClickModel()))\n",
    "    print \"         Balanced Interleaving with SDCM =      {0}%\".format(compare(data_set_2, BalancedInterleaver(), SimplifiedDependentClickModel()))\n",
    "    print \"         Probabilistic Interleaving with SDCM = {0}%\".format(compare(data_set_2, ProbabilisticInterleaver(), SimplifiedDependentClickModel()))\n",
    "    print \"     This data set contained {0} pairs.\".format(len(data_set_2))\n",
    "    print\n",
    "\n",
    "    print \"     On the data set where the experimental algorithm wins on all pairs according to normalized DCG at rank 5:\"\n",
    "    data_set_3 = generate_all_winners(RankingPair.delta_ndcg_at, 5)\n",
    "\n",
    "    print \"         Balanced Interleaving with RCM =       {0}%\".format(compare(data_set_3, BalancedInterleaver(), RandomClickModel()))\n",
    "    print \"         Probabilistic Interleaving with RCM =  {0}%\".format(compare(data_set_3, ProbabilisticInterleaver(), RandomClickModel()))\n",
    "    print \"         Balanced Interleaving with SDCM =      {0}%\".format(compare(data_set_3, BalancedInterleaver(), SimplifiedDependentClickModel()))\n",
    "    print \"         Probabilistic Interleaving with SDCM = {0}%\".format(compare(data_set_3, ProbabilisticInterleaver(), SimplifiedDependentClickModel()))\n",
    "    print \"     This data set contained {0} pairs.\".format(len(data_set_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following models express a preference in % for the experimental algorithm on the following data sets:\n",
      "\n",
      "     On the data set where the experimental algorithm wins on all pairs according to the average precision:\n",
      "         Balanced Interleaving with RCM =       0.729954331062%\n",
      "         Probabilistic Interleaving with RCM =  0.711237553343%\n",
      "         Balanced Interleaving with SDCM =      36.0372838212%\n",
      "         Probabilistic Interleaving with SDCM = 35.932469866%\n",
      "     This data set contained 26714 pairs.\n",
      "\n",
      "     On the data set where the experimental algorithm wins on all pairs according to DCG at rank 5:\n",
      "         Balanced Interleaving with RCM =       -0.30637254902%\n",
      "         Probabilistic Interleaving with RCM =  -0.779547930283%\n",
      "         Balanced Interleaving with SDCM =      40.0939542484%\n",
      "         Probabilistic Interleaving with SDCM = 39.6650326797%\n",
      "     This data set contained 29376 pairs.\n",
      "\n",
      "     On the data set where the experimental algorithm wins on all pairs according to normalized DCG at rank 5:\n",
      "         Balanced Interleaving with RCM =       -0.0238289760349%\n",
      "         Probabilistic Interleaving with RCM =  0.309776688453%\n",
      "         Balanced Interleaving with SDCM =      41.5339052288%\n",
      "         Probabilistic Interleaving with SDCM = 39.2531318083%\n",
      "     This data set contained 29376 pairs.\n"
     ]
    }
   ],
   "source": [
    "analyse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Let's explain our experiment design. Our goal is to do research on the extent to which offline and online evaluations agree with each other. To do this, we wanted to get as close to the 'we are a commercial search engine wanting to test an improvement to our algorithm'-narrative.\n",
    "\n",
    "In this narrative:\n",
    " - we have a proven production algorithm P;\n",
    " - we have a hopefully improved version of this algorithm, called E for experimental;\n",
    " - we have already seen that E outperforms P in our offline evaluations;\n",
    " - we want to know whether E is also better in 'the real world', using interleaving experiments with real users.\n",
    " \n",
    "Of course, we only simulate this narrative, so:\n",
    " - we use imaginary queries and pre-select the resulting rankings to those queries such that they match the narrative of E outperforming P in the offline experiments;\n",
    " - we use a click model to simulate user experiments, trained on real Yandex data.\n",
    " \n",
    "We do research on three different offline evaluation metrics. This means we 'play the narrative three times', have three different data sets. The offline evaluation metrics we wanted to research are:\n",
    "- average precision, because it is non-parametric;\n",
    "- DCG @ rank 5, because it is parametric\n",
    "- nDCG @ rank 5, to check whether it indeeds yields comparable results with respect to the unnormalized variant.\n",
    "\n",
    "As interleaving methods, we use the most straightforward on and an important probabalistic, more sophisticated variant:\n",
    "- balanced interleaving\n",
    "- probabilistic interleaving\n",
    "\n",
    "As a click model, we use a 'real' click model, and a random baseline:\n",
    "- random click model\n",
    "- simplified dependent click model\n",
    "\n",
    "This leaves us with 4 experiments on 3 different data sets, so 12 experiments in total.\n",
    "\n",
    "Staying true to our 'real world' narrative, we perform an interleaving experiment by regarding each ranking pair in the data set as an actual user that types in an actual query, getting one interleaved result from P and E, and then responding with one click behaviour (possibly encompassing multiple clicks). This means we do not, for instance, average over multiple interleaving instances for each query. While this would smooth out randomness, possibly yielding better results, this would not lead us to a realistic simulation of how online evaluation is used in the real world.\n",
    "\n",
    "After an interleaving experiment has been performed for each of the ranking pairs in the data set, we summarize the preference it expresses for the experimental algorithm via the metric proposed by Chapelle et al. in \"Large-scale validation and analysis of interleaved search evaluation\". We adjust the measure such that it is on a range from -100% to 100%. A score of 100% would mean that the online evaluation is totally in favour of the experimental algorithm.\n",
    "\n",
    "We expect strong preferences for the experimental algorithm with SDCM on all three data sets, since all three data sets are heavily biased towards the experimental algorithm via the offline measure. However, the results on the data sets selected by DCG and nDCG should be even more equal, since those are essentially the same metrics.\n",
    "\n",
    "We expect balanced interleaving or probabilistic interleaving to not give very different results. The small bias present in the balanced interleaving method should only manifest in much more fine-grained experiments where both algorithms give almost the same results but in a slightly different order; these subtleties should not be very visible in our case, since we have abstracted away from individual documents in the form of just three relevance grades.\n",
    "\n",
    "Lastly, any experiment using the random click model should give a score of about 0%.\n",
    "\n",
    "As we can see in the results section above, the scores match our expectations. It is interesting to see that the highest scores are around 40% instead of 100%; this is the effect of a lot of stochastic elements within the online evaluation simulation, but we did not expect it to be so large. Even on our highly biased data set, the online evaluation metrics seem quite insensitive.\n",
    "\n",
    "Our conclusion is that the online evaluation measures (except for the random click model, of course) agree with the offline evaluation metrics, but that they are somewhat insensitive.\n",
    "\n",
    "Missing in our research is a thorough assessment of statistical significance. This could be the subject of future research.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
